{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d051998",
   "metadata": {},
   "source": [
    "# MNIST DATA SET PREDICTION USING A SIMPLE MLP\n",
    "\n",
    "## Author: Moreen Owino \n",
    "\n",
    "Here i used my knowledge of neural networks to create a simple one layer MLP that is trained on the MNIST data set and is then used to predict the test image samples.It is coded from scratch using numpy.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80f1fe",
   "metadata": {},
   "source": [
    "##  load the necessary libraries\n",
    "start by loading the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fafd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for doing calculations etc\n",
    "import pandas as pd # for importing data sets\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt #for plotting digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea301a39",
   "metadata": {},
   "source": [
    "## load MNIST data sets\n",
    "The training and testing data sets are then loaded in below and the pandas data frames converted to numpy arrays to allow for numpy operations on the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ac40d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = pd.read_csv('mnist_train.csv')\n",
    "x_test = pd.read_csv('mnist_test.csv')\n",
    "y_targets = pd.read_csv('mnist_train_targets.csv')\n",
    "xtrain = xtrain.to_numpy()\n",
    "x_test = x_test.to_numpy() # converting to numpy arrays\n",
    "y_targets = y_targets.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd3752",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da109079",
   "metadata": {},
   "source": [
    "### normalizing the data\n",
    "\n",
    "The training data and test data contain images that are typically flattened into  one-dimensional arrays of length 784 (28x28=784)pixels.To normalize the pixel values to a range between 0 and 1, it is common to divide each element in the Xtrain and x_test array  by 255, which is the maximum possible pixel value.so we go ahead and divide the arrays by 255 in order to normalize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51938281",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99007a2",
   "metadata": {},
   "source": [
    "### one-hot encoding the target matrix\n",
    "We then one-hot encode the MNIST train targets to prepare them for the classification task. The target values are integers between 0 and 9, representing the actual digit in the corresponding image. If we use these integers as they are, the machine learning model might interpret them as continuous variables, which could lead to incorrect predictions. One-hot encoding converts each integer target value into a binary vector of length equal to the number of classes (i.e., 10 for MNIST). Each element of the vector is either 0 or 1, and corresponds to a specific class. The element corresponding to the actual class is set to 1, while all other elements are set to 0.The one hot conversion is done in the code chunk below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6d69b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values=10 # number of unique values or digits\n",
    "numbers=y_targets.shape[0] # getting the number of rows in the targets matrix\n",
    "\n",
    "y=y_targets.reshape(1, numbers)\n",
    "y_new=np.eye(values)[y.astype('int32')] #  uses advanced indexing to create a new array of shape (y.shape[0], values) where, \n",
    "# each row is a one-hot encoded representation of the corresponding element in y.\n",
    "\n",
    "targets=y_new.T.reshape(values, numbers)# Transforming the matrix so that we have the one one hot encoded digit per column.\n",
    "np.shape(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7737fe9",
   "metadata": {},
   "source": [
    "### Shuffling the training data\n",
    "This shuffling operation is often used in machine learning to randomize the order of training samples in order to prevent the model from learning patterns that may be specific to the order of the samples. By shuffling the data, the model is forced to learn general patterns that apply to all training samples, which can lead to better generalization performance on new, unseen data.\n",
    "In the code below random.permutation is used to generate the shuffled indexes that are then used to form the new arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8732bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "np.random.seed(138)\n",
    "k=60000 # number of columns of image samples\n",
    "shuffle_index=np.random.permutation(k) #shuffles the integers from 0 to k-1 randomly \n",
    "xtrain, targets = xtrain[:, shuffle_index], targets[:, shuffle_index] # new arrays with shuffled indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae3054",
   "metadata": {},
   "source": [
    "### plotting the data \n",
    "Let us plot a number just to have an idea of what kind of data we are trying to predict and to ensure that our data is all intact.we look at a random image i. i use the cmap=plt.cm.binary parameter to display the images in black and white, and use the axis('off') method to remove the axes labels.finaly plt.show is used to display the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9a015ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=3\n",
    "plt.imshow(xtrain[:,i].reshape(28,28), cmap = plt.cm.binary) # displays the images in black and white\n",
    "\n",
    "plt.axis(\"off\") # removes the axis labels\n",
    "plt.show # to display the figure\n",
    "print(targets[:,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddac740",
   "metadata": {},
   "source": [
    "## Defining the neural net architecture\n",
    "The Multilayer perceptrone  we built initially had an input layer with 784 neurones,one hidden layer with 64 neurones and an output layer with 10 neurones.It uses the sigmoid activation function in the hidden layer and softmax in the output layer.The MLP also utilizes mini batch gradient descent because this allows for a good balance between computation time and convergence to the optimal solution.The Adam optimizer  optimization algorithm for stochastic gradient descent (SGD), is used to update the parameters.The gradient of the loss function is estimated using a small batch of training examples at each iteration, rather than using the entire training set as in batch gradient descent. The Adam optimizer adapts the learning rate for each parameter based on the estimated first and second moments of the gradients, which allows it to converge faster and more reliably than traditional stochastic gradient descent methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e37349",
   "metadata": {},
   "source": [
    "## Functions\n",
    "We'll define some functions that we'll use in our MLP to make the MLP less cluttered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d29058",
   "metadata": {},
   "source": [
    "### *activation functions*\n",
    "These are the activation functions that we tried to use in the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "667cd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    h=1./(1. + np.exp(-z))\n",
    "    return h\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_prime(x):\n",
    "    s = softmax(x)\n",
    "    return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40370d",
   "metadata": {},
   "source": [
    "### *loss function*\n",
    "When training a neural network, the objective is to minimize the difference between the predicted output of the model and the true output of the input data. This difference is measured by a loss function that calculates the error between the predicted output and the actual output.The loss function is a measure of how well the neural network is performing in its task\n",
    "\n",
    "The function below was used to measure the loss after each epoch of training.The cross-entropy loss is computed as the negative average log-likelihood over all training samples where the average log-likelihood is computed by dividing the sum of log-likelihoods by the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87728efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m is the number of samples or instances in the dataset.\n",
    "# sum_loss calculates the sum of element-wise products of the true output y and the natural logarithm of the predicted output y_hat.\n",
    "# The negative sign and division by m scales the loss value appropriately.\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    sum_loss = np.sum(np.multiply(y, np.log(y_hat)))\n",
    "    m = y.shape[1]\n",
    "    l = -(1./m) *sum_loss\n",
    "    \n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c88c0",
   "metadata": {},
   "source": [
    "### forward pass function\n",
    "This function performs a feed-forward pass through a one-layer neural network with sigmoid activation function in the hidden layer and softmax activation function in the output layer.\n",
    "\n",
    "The function takes in three arguments X, Y, and params. X is the input data, Y is the ground truth labels, and params is a dictionary containing the model parameters W1, W2, b1, and b2.The function returns a dictionary cache that contains the intermediate results computed during the feed-forward pass, i.e Z1, A1, Z2, A2, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86f05909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X,Y, params):\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "    cache[\"A1\"] = sigmoid(cache[\"Z1\"])\n",
    "    cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "    cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "    cache[\"accuracy\"] = ((Y - cache[\"A2\"])/m) * 100\n",
    "    print(\"accuracy {}\".format(accuracy))\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6925e",
   "metadata": {},
   "source": [
    "### Backward pass function\n",
    "The function takes in four arguments X, Y, params, and cache. X is the input data, Y is the ground truth labels, params is a dictionary containing the model parameters W1, W2, b1, and b2, and cache is a dictionary containing the intermediate results computed during the feed-forward pass.The function computes the gradients of the cost function with respect to the model parameters W1, W2, b1, and b2 using backpropagation.\n",
    "The function first computes the error in the output layer dZ2 then computes the error in the hidden layer dA1 and gradients of each layer.Finally, the function returns a dictionary grads containing the gradients of the cost function with respect to the model parameters W1, W2, b1, and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbf8d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, Y, params, cache):\n",
    "\n",
    "    dZ2 = cache[\"A2\"] - Y\n",
    "    dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "    db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid(cache[\"Z1\"]) * (1 - sigmoid(cache[\"Z1\"]))\n",
    "    dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead66e03",
   "metadata": {},
   "source": [
    "## The train and predict function\n",
    "The function takes in train data, target data, test data, batch size, learning rate, input layer size, hidden layer size, output layer size and activation function as arguments to be used in the training process.It then implements a minibatch gradient descent algorithm with momentum. \n",
    "- It first initializes the weights and biases using the Glorot initialization method and then also initializes the adam optimizer variables.These variables are used to keep track of the historical gradients to update the current gradients.\n",
    "- the different activation functions that can be used in the MLP are then defined\n",
    "- we then have the forward and backward pass functions as disussed above\n",
    "- it then sets the momentum parameter to 0.9. This value determines how much of the previous gradient update is incorporated into the current update.\n",
    "- It the generates the batches and  iterates over the batches in the current epoch going through the forward and backwaard pass, updating the momemntum estimates for each of the four sets of parameters in the neural network and then updates the values of the weights and biases.It does this 9 times and the final values of weights and biases are used to predict the test set using the forward pass.\n",
    "- It also calculates the loss and accuracy of prediction at each iteration which helps to know whether the model is working properly.\n",
    "- it finaly returns the values of the predicted test set by using the forward pass function on the test set with the updated parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0881cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_pred(xtrain, targets, x_test, batch_size, learning_rate, input_size, h1_size, output_size, act_func,epochs ):\n",
    "    import numpy as np\n",
    "    \n",
    "    np.random.seed(138)\n",
    "    \n",
    "    m = xtrain.shape[1]\n",
    "    beta = .9\n",
    "        \n",
    "    num_batches = -(-m // batch_size) # Calculating the number of batches by dividing the number of columns in the train dataset,\n",
    "    #by batch_size\n",
    "        \n",
    "    \n",
    "    # initialization of weights and biases:Glorot initialization\n",
    "    params = { \"W1\": np.random.randn(h1_size, input_size) * np.sqrt(1. / input_size),\n",
    "           \"b1\": np.zeros((h1_size, 1)) * np.sqrt(1. / input_size),\n",
    "           \"W2\": np.random.randn(output_size, h1_size) * np.sqrt(1. / h1_size),\n",
    "           \"b2\": np.zeros((output_size, 1)) * np.sqrt(1. / h1_size) }\n",
    "        \n",
    "    # initialization of the adam optimization variables\n",
    "    V_dW1 = np.zeros(params[\"W1\"].shape)\n",
    "    V_db1 = np.zeros(params[\"b1\"].shape)\n",
    "    V_dW2 = np.zeros(params[\"W2\"].shape)\n",
    "    V_db2 = np.zeros(params[\"b2\"].shape)\n",
    "    \n",
    "    # the activation functions as described previously\n",
    "    def sigmoid(z):\n",
    "        h=1./(1. + np.exp(-z))\n",
    "        return h\n",
    "\n",
    "    def sigmoid_prime(z):\n",
    "            return sigmoid(z) * (1 - sigmoid(z))\n",
    " \n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    " \n",
    "    def softmax_prime(x):\n",
    "        s = softmax(x)\n",
    "        return np.diag(s) - np.outer(s, s)\n",
    "\n",
    "    def relu(x):\n",
    "        return max(0, x)\n",
    "\n",
    "    def relu_prime(x):\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    # the forward and backward pass functions as described previously\n",
    "        \n",
    "    def forward_pass(X, params):\n",
    "        cache = {}\n",
    "\n",
    "        cache[\"Z1\"] = np.matmul(params[\"W1\"], X) + params[\"b1\"]\n",
    "        cache[\"A1\"] = act_func(cache[\"Z1\"])\n",
    "        cache[\"Z2\"] = np.matmul(params[\"W2\"], cache[\"A1\"]) + params[\"b2\"]\n",
    "        cache[\"A2\"] = np.exp(cache[\"Z2\"]) / np.sum(np.exp(cache[\"Z2\"]), axis=0)\n",
    "       \n",
    "        return cache\n",
    "        \n",
    "    def backward_pass(X, Y, params, cache):\n",
    "        dZ2 = cache[\"A2\"] - Y\n",
    "        dW2 = (1./m_batch) * np.matmul(dZ2, cache[\"A1\"].T)\n",
    "        db2 = (1./m_batch) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = np.matmul(params[\"W2\"].T, dZ2)\n",
    "        dZ1 = dA1 * act_func(cache[\"Z1\"]) * (1 - act_func(cache[\"Z1\"]))\n",
    "        dW1 = (1./m_batch) * np.matmul(dZ1, X.T)\n",
    "        db1 = (1./m_batch) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "        return grads\n",
    "        \n",
    "        \n",
    "        # training the MLP\n",
    "        \n",
    "    for epoch in range(epochs):# setting the number of iterations to make\n",
    "        permutation = np.random.permutation(xtrain.shape[1]) # shuffling the data once again\n",
    "        X_train_shuffled = xtrain[:, permutation]\n",
    "        Y_train_shuffled = targets[:, permutation]\n",
    "        \n",
    "\n",
    "        for j in range(num_batches):\n",
    "            begin = j * batch_size # calculates the beginning index of the current batch\n",
    "            end = min(begin + batch_size, xtrain.shape[1] - 1) # calculates ending index of the current batch\n",
    "            X = X_train_shuffled[:, begin:end]\n",
    "            Y = Y_train_shuffled[:, begin:end]\n",
    "            m_batch = end - begin # calculates the number of samples in the current batch-used to adjust the learning rate during training\n",
    "\n",
    "            cache = forward_pass(X, params)\n",
    "            grads =backward_pass(X, Y, params, cache)\n",
    "            \n",
    "            # updating the parameters\n",
    "            V_dW1 = (beta * V_dW1 + (1. - beta) * grads[\"dW1\"])\n",
    "            V_db1 = (beta * V_db1 + (1. - beta) * grads[\"db1\"])\n",
    "            V_dW2 = (beta * V_dW2 + (1. - beta) * grads[\"dW2\"])\n",
    "            V_db2 = (beta * V_db2 + (1. - beta) * grads[\"db2\"])\n",
    "\n",
    "            params[\"W1\"] = params[\"W1\"] - learning_rate * V_dW1\n",
    "            params[\"b1\"] = params[\"b1\"] - learning_rate * V_db1\n",
    "            params[\"W2\"] = params[\"W2\"] - learning_rate * V_dW2\n",
    "            params[\"b2\"] = params[\"b2\"] - learning_rate * V_db2\n",
    "\n",
    "        # printing out the loss at each iteration to monitor the effectiveness of the MLP\n",
    "        cache = forward_pass(xtrain, params)\n",
    "        train_cost = loss(targets, cache[\"A2\"])\n",
    "        y_pred = np.argmax(cache[\"A2\"], axis=0)\n",
    "        Actual_y = np.argmax(targets, axis=0)\n",
    "        accuracy = np.mean(y_pred == Actual_y)*100\n",
    "        print(\"Epoch {}: training cost = {} : accuracy = {}\".format(epoch+1 ,train_cost,accuracy))\n",
    "        cache = forward_pass(x_test, params)\n",
    "        \n",
    "    # Using the foward pass function on the test set to get the predicted values\n",
    "    cache = forward_pass(x_test, params)    \n",
    "    predictions = np.argmax(cache[\"A2\"], axis=0)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a75f73",
   "metadata": {},
   "source": [
    "### *using the model to predict*\n",
    "You can then call the model and pass the required arguments as shown below and the ouput of the model will be the predictions of the test set data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7960c7aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training cost = 0.13731611262539065 : accuracy = 95.97833333333334\n",
      "Epoch 2: training cost = 0.09470939002892063 : accuracy = 97.13166666666667\n",
      "Epoch 3: training cost = 0.06810857816620425 : accuracy = 98.005\n",
      "Epoch 4: training cost = 0.05179004102916735 : accuracy = 98.495\n",
      "Epoch 5: training cost = 0.04290918971218689 : accuracy = 98.70166666666667\n",
      "Epoch 6: training cost = 0.032445682961629696 : accuracy = 99.06\n",
      "Epoch 7: training cost = 0.0269376469293807 : accuracy = 99.25333333333334\n",
      "Epoch 8: training cost = 0.0208318754632121 : accuracy = 99.52333333333333\n",
      "Epoch 9: training cost = 0.018639968564259837 : accuracy = 99.58500000000001\n",
      "Epoch 10: training cost = 0.016855874613912934 : accuracy = 99.63333333333333\n",
      "Epoch 11: training cost = 0.01351614615860694 : accuracy = 99.76666666666667\n",
      "Epoch 12: training cost = 0.010725948231405779 : accuracy = 99.84333333333333\n",
      "Epoch 13: training cost = 0.008445059268753774 : accuracy = 99.90833333333333\n",
      "Epoch 14: training cost = 0.006973274910644465 : accuracy = 99.94166666666666\n",
      "Epoch 15: training cost = 0.005918783915364372 : accuracy = 99.96000000000001\n",
      "Epoch 16: training cost = 0.0057224984032573116 : accuracy = 99.95\n",
      "Epoch 17: training cost = 0.005605160108233497 : accuracy = 99.96833333333333\n",
      "Epoch 18: training cost = 0.004817047953641769 : accuracy = 99.97\n",
      "Epoch 19: training cost = 0.0033308949275660036 : accuracy = 99.99166666666667\n",
      "Epoch 20: training cost = 0.0029787922902561356 : accuracy = 99.99333333333334\n",
      "Epoch 21: training cost = 0.002684624196551497 : accuracy = 99.99666666666667\n",
      "Epoch 22: training cost = 0.002543902854835725 : accuracy = 99.995\n",
      "Epoch 23: training cost = 0.0022344869419077696 : accuracy = 99.99666666666667\n",
      "Epoch 24: training cost = 0.00219829336790116 : accuracy = 99.99833333333333\n",
      "Epoch 25: training cost = 0.001936777619032374 : accuracy = 99.99833333333333\n"
     ]
    }
   ],
   "source": [
    "test_pred = train_pred(xtrain, targets, x_test, batch_size=128, learning_rate=4, input_size=784, h1_size=100, output_size=10, act_func=sigmoid, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f6ebf",
   "metadata": {},
   "source": [
    "After making the predictions we store the data in a csv file according to the kaggle submission requirements as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f077fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Id\": range(1, 10001), \"Expected\": test_pred})\n",
    "df.to_csv(\"example_submission66.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea09bf",
   "metadata": {},
   "source": [
    "## Discussion on effectiveness of the MLP\n",
    "Running the mlp did not take a lot of time since it was processing in batches as compared to some of the models that we run in R iterating on a sample by sample basis which took considerable time.\n",
    "The activation functions that produced the best results were sigmoid and softmax i.e having sigmoid in the hidden layer and softmax in the outer layer.\n",
    "\n",
    "At the start the model had 784 nodes in the input layer and 64 nodes in the hidden layer.Running the mlp through 9 epochs produced an accuracy score of about 0.96966 and increasing the epochs to 18 led to a score of 0.973333.Then further increase of epochs led to little or no improvement in score .Therefore we increased the nodes in the hidden layer to 100 and this led to better accuracy scores with our highest score of 0.98166 being the result of running the MLP with 784 input nodes, and 100 hidden layer nodes over 25 epochs.\n",
    "\n",
    "Experimentation was done with different batch sizes ,higher number of nodes in the hidden layer and different number of epoch runs but these did not produce better accuracy scores.Shuffling the data before training also lead to an increase in the score compared to using unshaffled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54383811",
   "metadata": {},
   "source": [
    "### Refferences\n",
    "- class notes\n",
    "\n",
    "- Getting started notebook on Kaggle "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
